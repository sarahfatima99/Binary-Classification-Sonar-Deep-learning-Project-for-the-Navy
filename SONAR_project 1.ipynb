{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as p\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 60)\n",
      "(208,)\n",
      "[[0.02   0.0371 0.0428 ... 0.0084 0.009  0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0522 0.0437 0.018  ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049  ... 0.0079 0.0036 0.0048]\n",
      " [0.026  0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M']\n"
     ]
    }
   ],
   "source": [
    "dataframe=p.read_csv(\"sonar.csv\",header=None)\n",
    "dataset=dataframe.values\n",
    "\n",
    "\n",
    "X=dataset[:,0:60].astype(float)\n",
    "Y=dataset[:,60]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "encoded_y = labelencoder.fit_transform(Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sarah Fatima\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Sarah Fatima\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Results:81.68%(7.32%)\n",
      "standardize:82.64%(8.75%)\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu',  input_shape=(60,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')  )    \n",
    "    from keras import optimizers\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "estimator= KerasClassifier(build_fn=build_model,epochs=100,batch_size=5,verbose=0)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "results=cross_val_score(estimator,X,encoded_y,cv=kfold)\n",
    "print(\"Results:%.2f%%(%.2f%%)\"%(results.mean()*100,results.std()*100))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(seed)\n",
    "estimators=[]\n",
    "estimators.append(('standardize',StandardScaler()))\n",
    "estimators.append(('mlp',KerasClassifier(build_fn=build_model,epochs=100,batch_size=5,verbose=0)))\n",
    "pipeline=Pipeline(estimators)\n",
    "Kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "result=cross_val_score(pipeline,X,encoded_y,cv=Kfold)\n",
    "print(\"standardize:%.2f%%(%.2f%%)\"%(result.mean()*100,result.std()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMALLER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): #DONOT USE BOTTEL NECK; IE DONOT USE SMALLER WIEGHS THEN LARGER THEN SMALLER, DECREASE WOEGHS IN SEGWUENCE\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(3, activation='relu',  input_shape=(60,)))\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')  )    \n",
    "    from keras import optimizers\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "estimator= KerasClassifier(build_fn=build_model,epochs=100,batch_size=5,verbose=0)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "results=cross_val_score(estimator,X,encoded_y,cv=kfold)\n",
    "print(\"Results:%.2f%%(%.2f%%)\"%(results.mean()*100,results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LARGER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, activation='relu',  input_shape=(60,)))\n",
    "    model.add(Dense(60, activation='relu'))  \n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')  )    \n",
    "    from keras import optimizers\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "estimator= KerasClassifier(build_fn=build_model,epochs=100,batch_size=5,verbose=0)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "results=cross_val_score(estimator,X,encoded_y,cv=kfold)\n",
    "print(\"Results:%.2f%%(%.2f%%)\"%(results.mean()*100,results.std()*100))\n",
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(605, activation='relu',  input_shape=(60,)))\n",
    "    model.add(Dense(608, activation='relu'))  \n",
    "    model.add(Dense(309, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')  )    \n",
    "    from keras import optimizers\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "estimator= KerasClassifier(build_fn=build_model,epochs=500,batch_size=5,verbose=0)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "results=cross_val_score(estimator,X,encoded_y,cv=kfold)\n",
    "print(\"Results:%.2f%%(%.2f%%)\"%(results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# over fits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, activation='relu',  input_shape=(60,)))\n",
    "    model.add(Dense(30, activation='relu'))  \n",
    "    model.add(Dense(1, activation='sigmoid')  )    \n",
    "    from keras import optimizers\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "estimator= KerasClassifier(build_fn=build_model,epochs=100,batch_size=5,verbose=0)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "results=cross_val_score(estimator,X,encoded_y,cv=kfold)\n",
    "print(\"Results:%.2f%%(%.2f%%)\"%(results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6059, activation='relu',  input_shape=(60,)))\n",
    "    model.add(Dense(698, activation='relu'))  \n",
    "    model.add(Dense(3090, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')  )    \n",
    "    from keras import optimizers\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "estimator= KerasClassifier(build_fn=build_model,epochs=500,batch_size=5,verbose=0)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "results=cross_val_score(estimator,X,encoded_y,cv=kfold)\n",
    "print(\"Results:%.2f%%(%.2f%%)\"%(results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  FUNCTIONAL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "def build_model():\n",
    "    input_tensor = Input(shape=(60,))\n",
    "    x = Dense(74, activation='relu')(input_tensor)\n",
    "    x = Dense(63, activation='relu')(x)\n",
    "    output_tensor = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "estimator= KerasClassifier(build_fn=build_model,epochs=100,batch_size=5,verbose=0)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "results=cross_val_score(estimator,X,encoded_y,cv=kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(Y)\n",
    "np.random.shuffle(X)\n",
    "data=Y\n",
    "string = 'MR'\n",
    "char_to_int = dict((c, i) for i, c in enumerate(string))\n",
    "y= [char_to_int[char] for char in data]\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu',  input_shape=(60,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')  )    \n",
    "    from keras import optimizers\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "(156, 60)\n",
      "WARNING:tensorflow:From C:\\Users\\Sarah Fatima\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Sarah Fatima\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "39/39 [==============================] - 0s 6ms/step\n",
      "processing fold # 1\n",
      "(156, 60)\n",
      "39/39 [==============================] - 0s 6ms/step\n",
      "processing fold # 2\n",
      "(156, 60)\n",
      "39/39 [==============================] - 0s 6ms/step\n",
      "processing fold # 3\n",
      "(156, 60)\n",
      "39/39 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "xtrain=X[:156]\n",
    "ytrain=y[:156]\n",
    "k=4\n",
    "num_val_samples = len(xtrain)//k\n",
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = xtrain[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    print(xtrain.shape)\n",
    "    val_targets = ytrain[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "    [xtrain[:i * num_val_samples],\n",
    "    xtrain[(i + 1) * num_val_samples:]],\n",
    "    axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "    [ytrain[:i * num_val_samples],\n",
    "    ytrain[(i + 1) * num_val_samples:]],\n",
    "    axis=0)\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "    epochs=num_epochs, batch_size=5,verbose=0)\n",
    "    results = model.evaluate(val_data,val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.6977 - acc: 0.4615\n",
      "Epoch 2/160\n",
      "52/52 [==============================] - 0s 102us/step - loss: 0.6927 - acc: 0.4615\n",
      "Epoch 3/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6901 - acc: 0.5000\n",
      "Epoch 4/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6880 - acc: 0.5192\n",
      "Epoch 5/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6862 - acc: 0.5769\n",
      "Epoch 6/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6847 - acc: 0.5577\n",
      "Epoch 7/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6832 - acc: 0.5577\n",
      "Epoch 8/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6820 - acc: 0.5385\n",
      "Epoch 9/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6808 - acc: 0.5385\n",
      "Epoch 10/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6796 - acc: 0.5192\n",
      "Epoch 11/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6785 - acc: 0.5385\n",
      "Epoch 12/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6775 - acc: 0.5192\n",
      "Epoch 13/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.6765 - acc: 0.5577\n",
      "Epoch 14/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6756 - acc: 0.5385\n",
      "Epoch 15/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6746 - acc: 0.5577\n",
      "Epoch 16/160\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.6737 - acc: 0.5577\n",
      "Epoch 17/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.6727 - acc: 0.5577\n",
      "Epoch 18/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6717 - acc: 0.5577\n",
      "Epoch 19/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6707 - acc: 0.5962\n",
      "Epoch 20/160\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.6696 - acc: 0.5769\n",
      "Epoch 21/160\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.6687 - acc: 0.5962\n",
      "Epoch 22/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6676 - acc: 0.5962\n",
      "Epoch 23/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6666 - acc: 0.5962\n",
      "Epoch 24/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6656 - acc: 0.5962\n",
      "Epoch 25/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6645 - acc: 0.5962\n",
      "Epoch 26/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6633 - acc: 0.6154\n",
      "Epoch 27/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6623 - acc: 0.6346\n",
      "Epoch 28/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6612 - acc: 0.6346\n",
      "Epoch 29/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6601 - acc: 0.6154\n",
      "Epoch 30/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6590 - acc: 0.6346\n",
      "Epoch 31/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6578 - acc: 0.6346\n",
      "Epoch 32/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.6566 - acc: 0.6538\n",
      "Epoch 33/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6553 - acc: 0.6538\n",
      "Epoch 34/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6541 - acc: 0.6538\n",
      "Epoch 35/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6530 - acc: 0.6538\n",
      "Epoch 36/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6518 - acc: 0.6538\n",
      "Epoch 37/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6507 - acc: 0.6731\n",
      "Epoch 38/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6495 - acc: 0.6538\n",
      "Epoch 39/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6483 - acc: 0.6923\n",
      "Epoch 40/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.6471 - acc: 0.6538\n",
      "Epoch 41/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6459 - acc: 0.7115\n",
      "Epoch 42/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6447 - acc: 0.6923\n",
      "Epoch 43/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6434 - acc: 0.7115\n",
      "Epoch 44/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6422 - acc: 0.7115\n",
      "Epoch 45/160\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.6408 - acc: 0.7115\n",
      "Epoch 46/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.6395 - acc: 0.7115\n",
      "Epoch 47/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6383 - acc: 0.7115\n",
      "Epoch 48/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6371 - acc: 0.7115\n",
      "Epoch 49/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6357 - acc: 0.7115\n",
      "Epoch 50/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.6343 - acc: 0.7115\n",
      "Epoch 51/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6330 - acc: 0.7308\n",
      "Epoch 52/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6317 - acc: 0.7115\n",
      "Epoch 53/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6306 - acc: 0.7308\n",
      "Epoch 54/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.6293 - acc: 0.7115\n",
      "Epoch 55/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6278 - acc: 0.7308\n",
      "Epoch 56/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6265 - acc: 0.7115\n",
      "Epoch 57/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6252 - acc: 0.7308\n",
      "Epoch 58/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6239 - acc: 0.7115\n",
      "Epoch 59/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6226 - acc: 0.7308\n",
      "Epoch 60/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6212 - acc: 0.7115\n",
      "Epoch 61/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6197 - acc: 0.7115\n",
      "Epoch 62/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6182 - acc: 0.7115\n",
      "Epoch 63/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6167 - acc: 0.7115\n",
      "Epoch 64/160\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.6152 - acc: 0.7500\n",
      "Epoch 65/160\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.6137 - acc: 0.6923\n",
      "Epoch 66/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6122 - acc: 0.7500\n",
      "Epoch 67/160\n",
      "52/52 [==============================] - 0s 96us/step - loss: 0.6106 - acc: 0.6923\n",
      "Epoch 68/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6091 - acc: 0.7500\n",
      "Epoch 69/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6075 - acc: 0.6923\n",
      "Epoch 70/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6058 - acc: 0.7500\n",
      "Epoch 71/160\n",
      "52/52 [==============================] - 0s 96us/step - loss: 0.6041 - acc: 0.6923\n",
      "Epoch 72/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.6026 - acc: 0.7500\n",
      "Epoch 73/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.6011 - acc: 0.6923\n",
      "Epoch 74/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5995 - acc: 0.7500\n",
      "Epoch 75/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5980 - acc: 0.6923\n",
      "Epoch 76/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5964 - acc: 0.7500\n",
      "Epoch 77/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5948 - acc: 0.7308\n",
      "Epoch 78/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5933 - acc: 0.7308\n",
      "Epoch 79/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5916 - acc: 0.7308\n",
      "Epoch 80/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5900 - acc: 0.7308\n",
      "Epoch 81/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5886 - acc: 0.7308\n",
      "Epoch 82/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5873 - acc: 0.7500\n",
      "Epoch 83/160\n",
      "52/52 [==============================] - 0s 96us/step - loss: 0.5856 - acc: 0.7308\n",
      "Epoch 84/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5841 - acc: 0.7500\n",
      "Epoch 85/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5824 - acc: 0.7500\n",
      "Epoch 86/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5808 - acc: 0.7308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5791 - acc: 0.7500\n",
      "Epoch 88/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5775 - acc: 0.7500\n",
      "Epoch 89/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5760 - acc: 0.7500\n",
      "Epoch 90/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5744 - acc: 0.7500\n",
      "Epoch 91/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5728 - acc: 0.7500\n",
      "Epoch 92/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5711 - acc: 0.7500\n",
      "Epoch 93/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5695 - acc: 0.7500\n",
      "Epoch 94/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5679 - acc: 0.7885\n",
      "Epoch 95/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5661 - acc: 0.7500\n",
      "Epoch 96/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5645 - acc: 0.7885\n",
      "Epoch 97/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5628 - acc: 0.7692\n",
      "Epoch 98/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5613 - acc: 0.7885\n",
      "Epoch 99/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5595 - acc: 0.7692\n",
      "Epoch 100/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5581 - acc: 0.7885\n",
      "Epoch 101/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5564 - acc: 0.7692\n",
      "Epoch 102/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.5549 - acc: 0.7885\n",
      "Epoch 103/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5531 - acc: 0.7692\n",
      "Epoch 104/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.5515 - acc: 0.7885\n",
      "Epoch 105/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5497 - acc: 0.7692\n",
      "Epoch 106/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5483 - acc: 0.7885\n",
      "Epoch 107/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5464 - acc: 0.7692\n",
      "Epoch 108/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5447 - acc: 0.7885\n",
      "Epoch 109/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.5432 - acc: 0.7692\n",
      "Epoch 110/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5417 - acc: 0.8077\n",
      "Epoch 111/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5400 - acc: 0.7692\n",
      "Epoch 112/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5383 - acc: 0.8077\n",
      "Epoch 113/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5367 - acc: 0.7692\n",
      "Epoch 114/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5349 - acc: 0.8077\n",
      "Epoch 115/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5332 - acc: 0.7692\n",
      "Epoch 116/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5316 - acc: 0.8077\n",
      "Epoch 117/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5299 - acc: 0.7885\n",
      "Epoch 118/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5283 - acc: 0.8077\n",
      "Epoch 119/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.5265 - acc: 0.7885\n",
      "Epoch 120/160\n",
      "52/52 [==============================] - 0s 39us/step - loss: 0.5248 - acc: 0.8077\n",
      "Epoch 121/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5230 - acc: 0.7885\n",
      "Epoch 122/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.5213 - acc: 0.8077\n",
      "Epoch 123/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5196 - acc: 0.7885\n",
      "Epoch 124/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5181 - acc: 0.8269\n",
      "Epoch 125/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5166 - acc: 0.8077\n",
      "Epoch 126/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5147 - acc: 0.8269\n",
      "Epoch 127/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5128 - acc: 0.8077\n",
      "Epoch 128/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5109 - acc: 0.8269\n",
      "Epoch 129/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5090 - acc: 0.8077\n",
      "Epoch 130/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5071 - acc: 0.8269\n",
      "Epoch 131/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5052 - acc: 0.8077\n",
      "Epoch 132/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5035 - acc: 0.8269\n",
      "Epoch 133/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.5020 - acc: 0.8077\n",
      "Epoch 134/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.5002 - acc: 0.8269\n",
      "Epoch 135/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4988 - acc: 0.7885\n",
      "Epoch 136/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4971 - acc: 0.8269\n",
      "Epoch 137/160\n",
      "52/52 [==============================] - 0s 96us/step - loss: 0.4951 - acc: 0.8462\n",
      "Epoch 138/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4937 - acc: 0.8269\n",
      "Epoch 139/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4916 - acc: 0.8462\n",
      "Epoch 140/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4903 - acc: 0.8462\n",
      "Epoch 141/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4887 - acc: 0.8462\n",
      "Epoch 142/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4874 - acc: 0.8462\n",
      "Epoch 143/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4857 - acc: 0.8462\n",
      "Epoch 144/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4843 - acc: 0.8462\n",
      "Epoch 145/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4823 - acc: 0.8462\n",
      "Epoch 146/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4811 - acc: 0.8462\n",
      "Epoch 147/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4793 - acc: 0.8462\n",
      "Epoch 148/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4779 - acc: 0.8462\n",
      "Epoch 149/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4762 - acc: 0.8462\n",
      "Epoch 150/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4747 - acc: 0.8462\n",
      "Epoch 151/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4731 - acc: 0.8462\n",
      "Epoch 152/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4716 - acc: 0.8462\n",
      "Epoch 153/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4703 - acc: 0.8462\n",
      "Epoch 154/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4688 - acc: 0.8462\n",
      "Epoch 155/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4671 - acc: 0.8462\n",
      "Epoch 156/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4657 - acc: 0.8462\n",
      "Epoch 157/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4643 - acc: 0.8269\n",
      "Epoch 158/160\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.4627 - acc: 0.8462\n",
      "Epoch 159/160\n",
      "52/52 [==============================] - 0s 38us/step - loss: 0.4613 - acc: 0.8269\n",
      "Epoch 160/160\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.4595 - acc: 0.8462\n",
      "52/52 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu',  input_shape=(60,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')  )    \n",
    "    from keras import optimizers\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "model=build_model()\n",
    "model.fit(X[156:], y[156:], epochs=160, batch_size=512)\n",
    "results = model.evaluate(X[156:], y[156:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SUBCLASSING\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
